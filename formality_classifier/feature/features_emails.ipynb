{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook computes all the features from the hotel e-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORTATIONS\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from joblib import load, dump\n",
    "import pickle\n",
    "\n",
    "import spacy, collections, readability\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ne_chunk, pos_tag, ngrams\n",
    "from pickle import dump, load\n",
    "from nltk.parse import CoreNLPParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of the e-mails: convert into a list of sentence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_content(html_file):\n",
    "    file = open(html_file, 'r', encoding = 'utf-8')\n",
    "    soup = BeautifulSoup(file)        \n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    strips = list(soup.stripped_strings)\n",
    "    return strips \n",
    "\n",
    "\n",
    "def process_emails(html_dir): ##input is the path to the html directory containing all of the html files\n",
    "    html_files_path = [] x for x in os.listdir(html_dir) if 'html' in x]\n",
    "    for html_file in html_files_path:\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Users\\\\wyannis\\\\Documents\\\\Edinburgh\\\\Dissertation\\\\Python_codes\\\\text-formality-classifier\\\\feature'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-34a226dcff04>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-34a226dcff04>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    sent_list =\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sent_list = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions used to compute the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Intermediary functions\n",
    "\n",
    "def get_case(string):\n",
    "    \"\"\"\n",
    "    return a number list containing: the number of entirely-capitalized words; binary indicator for whether sentence\n",
    "    is lowercase; binary indicator for whether the first word is capitalized.\n",
    "    \"\"\"\n",
    "    return [len([w for w in word_tokenize(string) if w.isupper()]), int(string.islower()), int(string[0].isupper())]\n",
    "\n",
    "def get_lexical(string):\n",
    "    \"\"\"\n",
    "    return the number of contractions in the sentence, normalized by length; average word length; average word\n",
    "    log-frequency according to Google Ngram corpus; average formality score as computed by Pavlick and\n",
    "    Nenkova (2015).\n",
    "    \"\"\"\n",
    "    # prepare\n",
    "    output = []\n",
    "    tokens = word_tokenize(string.lower())\n",
    "    length = len(tokens)\n",
    "\n",
    "    # 1. number of contractions, norm by length\n",
    "    cont_count = 0\n",
    "    for w in tokens:\n",
    "        if \"\\'\" in w and len(w) > 1:\n",
    "            cont_count += 1\n",
    "    output.append(round(cont_count/length, 2))\n",
    "\n",
    "    # 2. average word length\n",
    "    output.append(round(sum([len(w) for w in tokens])/length, 2))\n",
    "    \n",
    "    return output\n",
    "    \n",
    "def get_punctuation_number(string):\n",
    "    \"\"\"\n",
    "    punctuation Number of ‘?’, ‘...’, and ‘!’ in the sentence.\n",
    "    \"\"\"\n",
    "    punct_number = 0\n",
    "    tokens = word_tokenize(string)\n",
    "\n",
    "    for w in tokens:\n",
    "        if w in [\"?\", \"...\", \"!\"]:\n",
    "            punct_number += 1\n",
    "    return [punct_number]\n",
    "\n",
    "\n",
    "## Final function gathering all those stats\n",
    "\n",
    "def get_fast_num_stats(string):\n",
    "    # get the easier-to-compute features\n",
    "    output = []\n",
    "    # 3 nums\n",
    "    output.extend(get_case(string))\n",
    "    # 2 nums\n",
    "    output.extend(get_lexical(string))\n",
    "    # 1 num\n",
    "    output.extend(get_punctuation_number(string))\n",
    "\n",
    "    # total of 6 nums per input sentence\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_fast_number_features(sent_list):\n",
    "    \"\"\"\n",
    "    return an array of an array of features for each sentence\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return load(open(FAST_NUM_FEAT_PATH, \"rb\"))\n",
    "    except IOError:\n",
    "        print(\"Cannot load number vectors, now extracting feature...\")\n",
    "\n",
    "        # get list of raw number stats for each sentence\n",
    "        print(\"getting raw features...\")\n",
    "        num_lists = []\n",
    "        for s in sent_list:\n",
    "            num_lists.append(simple_stat.get_fast_num_stats(s))\n",
    "        print(\"features created\")\n",
    "\n",
    "        # list into vec array\n",
    "        output = np.array(num_lists)\n",
    "\n",
    "        # save the feature vectores\n",
    "        dump(output, FAST_NUM_FEAT_PATH)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_case returns: [1, 0, 1]\n",
      "get_lexical returns: [0.0, 4.46]\n",
      "get_punctuation_number returns: [1]\n"
     ]
    }
   ],
   "source": [
    "## TEST OF FUNCTIONS\n",
    "\n",
    "test_string = \"I do not approve of this terrible dissertation, it absolutely sucks !\"\n",
    "print(f\"get_case returns: {get_case(test_string)}\")\n",
    "print(f\"get_lexical returns: {get_lexical(test_string)}\")\n",
    "print(f\"get_punctuation_number returns: {get_punctuation_number(test_string)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computation of the features arrays for the e-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emails_features(sent_list):\n",
    "    res = []\n",
    "    for sent in sent_list:\n",
    "        res.append(get_fast_num_stats(sent))\n",
    "    res = np.array(res)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### first, let's transform each e-mail into its list of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_content(html_file):\n",
    "    file = open(html_file, 'r', encoding = 'utf-8')\n",
    "    soup = BeautifulSoup(file)        \n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    strips = list(soup.stripped_strings)\n",
    "    return strips \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRONG: consider that every sentence from an informal document is in fact informal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
