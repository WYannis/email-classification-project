{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the pre-processing steps:\n",
    "\n",
    "- clean the data (remove hashtag, mentions of accounts with @) and lift the different declinations of a word (each word is stemmed)\n",
    "- tokenization of the tweets and padding removal\n",
    "- proceed to the embedding of word tokens with the GloVe vectors\n",
    "- set up the embedding layers for the LSTM-CNN neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import re, sys, os, csv\n",
    "from many_stop_words import get_stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Concatenate, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning of the tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(get_stop_words('en'))         #About 900 stop words\n",
    "nltk_words = list(stopwords.words('english'))   #About 150 stop words\n",
    "stop_words.extend(nltk_words)\n",
    "\n",
    "def words(text): return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "dictionary = Counter(words(open('dataset/wordlists/merged.txt').read()))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))\n",
    "\n",
    "def clean_tweet( tweet):\n",
    "        tweet = tweet.lower()\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def remove_stopwords(word_list):\n",
    "        filtered_tweet=\"\"\n",
    "        for word in word_list:\n",
    "            word = word.lower() \n",
    "            if word not in stopwords.words(\"english\"):\n",
    "                filtered_tweet=filtered_tweet + \" \" + word        \n",
    "        return filtered_tweet.lstrip()\n",
    "    \n",
    "def change_label(label):\n",
    "    if label == \"empty\":return 0\n",
    "    elif label == \"sadness\":return 4\n",
    "    elif label == \"enthusiasm\":return 1\n",
    "    elif label == \"neutral\":return 0\n",
    "    elif label == \"worry\":return 3\n",
    "    elif label == \"surprise\":return 2\n",
    "    elif label == \"love\":return 2\n",
    "    elif label == \"fun\":return 2\n",
    "    elif label == \"hate\":return 4\n",
    "    elif label == \"happiness\":return 1\n",
    "    elif label == \"boredom\":return 0\n",
    "    elif label == \"relief\":return 2\n",
    "    elif label == \"anger\":return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet dataset shape: (40000, 4)\n",
      "empty : @tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('dataset/data/text_emotion.csv', sep=',')\n",
    "print(\"Tweet dataset shape:\",data_train.shape)\n",
    "print(data_train.sentiment[0],\":\",data_train.content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning process is completed! Clean data stored in data.csv\n"
     ]
    }
   ],
   "source": [
    "def rewrite_clean_data(dirty_data):\n",
    "    dataWriter = csv.writer(open('data.csv', 'w'), delimiter=',',lineterminator=\"\\n\")\n",
    "    for i in range(len(dirty_data)):\n",
    "        tweet= clean_tweet(dirty_data.content[i])\n",
    "        tweet = remove_stopwords(tweet.split())\n",
    "        if change_label(dirty_data.sentiment[i]) != 4:      #removal of anger, hate and sadness as unlikely to be found in marketing e-mails\n",
    "            dataWriter.writerow([tweet, str(change_label(dirty_data.sentiment[i]))])\n",
    "    print(\"Cleaning process is completed! Clean data stored in data.csv\")\n",
    "    \n",
    "#rewrite_clean_data(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for the embedding and pre-processing:\n",
      " 40000 35 0.3 200 \n",
      " dataset/glove/glove.twitter.27B.200d.txt\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 40000 # max no. of words for tokenizer\n",
    "MAX_SEQUENCE_LENGTH = 30 # max length of text (words) including padding\n",
    "testing_split = 0.3\n",
    "EMBEDDING_DIM = 200 # embedding dimensions for word vectors (word2vec/GloVe)\n",
    "GLOVE_DIR = \"dataset/glove/glove.twitter.27B.\"+str(200)+\"d.txt\"\n",
    "print(\"Parameters for the embedding and pre-processing:\\n\",\n",
    "      MAX_NB_WORDS,MAX_SEQUENCE_LENGTH+5,\n",
    "      testing_split,EMBEDDING_DIM,\"\\n\",\n",
    "      GLOVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the X,Y vectors to be used for the training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from the csv file...Done!\n",
      "Succesfully save the word tokenizer to file: tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "texts, labels = [], []\n",
    "print(\"Reading from the csv file...\", end=\"\")\n",
    "with open('data.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        texts.append(row[0])\n",
    "        labels.append(row[1])\n",
    "print(\"Done!\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pk.dump(tokenizer, handle, protocol=pk.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#We need to save the tokenizer because it will be re-used during the training phase of the neural network\n",
    "print(\"Succesfully save the word tokenizer to file: tokenizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27496 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data_int = pad_sequences(sequences, padding='pre', maxlen=(MAX_SEQUENCE_LENGTH-5))\n",
    "data = pad_sequences(data_int, padding='post', maxlen=(MAX_SEQUENCE_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (33402, 30)\n",
      "Shape of label tensor: (33402, 4)\n"
     ]
    }
   ],
   "source": [
    "labels = to_categorical(np.asarray(labels)) # convert to one-hot encoding vectors\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "# Shuffling step to make sure data is in a randomized order\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in each category:\n",
      "Training:\n",
      " [6733. 4161. 6548. 5939.]\n",
      "Testing:\n",
      " [2911. 1807. 2783. 2520.]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=testing_split, random_state=42)\n",
    "\n",
    "print('Number of entries in each category:')\n",
    "print(\"Training:\\n\",y_train.sum(axis=0))\n",
    "print(\"Testing:\\n\",y_test.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
